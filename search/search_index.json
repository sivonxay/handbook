{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Persson Group Handbook This website serves as the group handbook for the Persson Group at the University of California, Berkeley / Lawrence Berkeley National Laboratory. The purpose of this handbook is to provide information to new students and post-docs who do not yet have full access to internal group resources when they first join the group (hence why this handbook is publically available). This handbook is just the starting point to ensure a smoother integration of new group members and an efficient onboarding process. Our newest group members often have the most valuable insight in improving the onboarding process and where there may be critical gaps in the handbook. While Ann Rutt and Christian Legaspi have the ultimate responsibility of overseeing the handbook (in their group job capacity as \"Onboarding Guides\") all Persson Group members are encouraged to contribute to this handbook via the open-source repo.","title":"The Persson Group Handbook"},{"location":"#the-persson-group-handbook","text":"This website serves as the group handbook for the Persson Group at the University of California, Berkeley / Lawrence Berkeley National Laboratory. The purpose of this handbook is to provide information to new students and post-docs who do not yet have full access to internal group resources when they first join the group (hence why this handbook is publically available). This handbook is just the starting point to ensure a smoother integration of new group members and an efficient onboarding process. Our newest group members often have the most valuable insight in improving the onboarding process and where there may be critical gaps in the handbook. While Ann Rutt and Christian Legaspi have the ultimate responsibility of overseeing the handbook (in their group job capacity as \"Onboarding Guides\") all Persson Group members are encouraged to contribute to this handbook via the open-source repo.","title":"The Persson Group Handbook"},{"location":"acknowlegements/","text":"A large part of the material in the 'Getting Started' guide was adopted from the Jain Group Handbook , written by Anubhav Jain. Authors: John Dagdelen, Eric Sivonxay, Ann Rutt, Kara Fong Kara Fong wrote a great tutorial on getting started on NERSC, which we've also included here.","title":"Acknowlegements"},{"location":"about/about/","text":"About the Group Our group studies the physics and chemistry of materials using atomistic computational methods and high-performance computing technology. Most of our research is focused on materials for energy applications, such as battery electrode materials, electrolytes, photocatalysts, thermoelectrics, etc. We are co-located at Lawrence Berkeley National Laboratory (LBNL) and the University of California, Berkeley, just down the hill from LBNL. Our main workspaces are located at LBNL, but we have a number of additional desks available on campus for students to use in between classes, which are located in the grad bay of Hearst Memorial Mining Building. San Francisco is located across the bay from Berkeley and is about a 30 minute drive or BART train ride away. Berkeley itself is a vibrant city of 115,000 people filled with cafes, restaurants of all types, and cultural activities. Group Roster The group roster is available on the internal group site and the google drive Meetings Group Meeting There is a mandatory 1hr group meeting that occurs approximately every two weeks. At group meeting, one of the group memebers will give a talk on their research. You will be placed on a rotating schedule that will have you present your work about every 1 year. Subgroup Meetings Given our new additions and growth, we have decided to implement new subgroups, associated leaders, and members. Undergraduate students are welcome to join any subgroup they want, but be aware that subgroup meetings probably can't be moved around to fit undergraduate members' schedules. The responsibility of subgroup leads is to find a slot where everyone can make bi-weekly meetings and send out calendar calls and reminders. The current subgroups are (leads are in bold): Subgroups Materials Project: Donny Winston , Shyam Dwaraknath, Patrick Huck, John Dagdelen, David Mrdjenovich, Matthew Horton Spectroscopy: Shyam Dwaraknath , Handong Ling, Matthew McDermott Materials Design (photocatalysts, magnetocalorics, piezoelectrics, TCOs, etc): Joey Montoya , Shyam Dwaraknath, Rachel Woods-Robinson, Matthew Horton, Jordan Burns, Handong Ling Electrodes: Jimmy Shen , Eric Sivonxay, Jianli Cheng, Ann Rutt, Jordan Burns, Matthew Horton Synthesis: John Dagdelen , Rebecca Stern, David Mrdjenovich, Matthew McDermott, Matthew Horton Electrolytes-SEI: Trevor Seguin , Brandon Wood, Julian Self, Tingzheng Hou, Sam Blau, Kara Fong Group events Some of the regular things we do as a group are: Lunch every two weeks jointly with our group and members of the Jain research group. A group outing once every semester. Past activities included paintball and cardboard boat races in Kristin's pool. Twice-yearly \u201cCodeBusters\u201d events, where we spend 3 days hacking on code together. Dinners to celebrate group members who are moving on to join universities or companies. An annual joint holiday party with the Ceder Group.","title":"General"},{"location":"about/about/#about-the-group","text":"Our group studies the physics and chemistry of materials using atomistic computational methods and high-performance computing technology. Most of our research is focused on materials for energy applications, such as battery electrode materials, electrolytes, photocatalysts, thermoelectrics, etc. We are co-located at Lawrence Berkeley National Laboratory (LBNL) and the University of California, Berkeley, just down the hill from LBNL. Our main workspaces are located at LBNL, but we have a number of additional desks available on campus for students to use in between classes, which are located in the grad bay of Hearst Memorial Mining Building. San Francisco is located across the bay from Berkeley and is about a 30 minute drive or BART train ride away. Berkeley itself is a vibrant city of 115,000 people filled with cafes, restaurants of all types, and cultural activities.","title":"About the Group "},{"location":"about/about/#group-roster","text":"The group roster is available on the internal group site and the google drive","title":"Group Roster "},{"location":"about/about/#meetings","text":"","title":"Meetings "},{"location":"about/about/#group-meeting","text":"There is a mandatory 1hr group meeting that occurs approximately every two weeks. At group meeting, one of the group memebers will give a talk on their research. You will be placed on a rotating schedule that will have you present your work about every 1 year.","title":"Group Meeting "},{"location":"about/about/#subgroup-meetings","text":"Given our new additions and growth, we have decided to implement new subgroups, associated leaders, and members. Undergraduate students are welcome to join any subgroup they want, but be aware that subgroup meetings probably can't be moved around to fit undergraduate members' schedules. The responsibility of subgroup leads is to find a slot where everyone can make bi-weekly meetings and send out calendar calls and reminders. The current subgroups are (leads are in bold):","title":"Subgroup Meetings "},{"location":"about/about/#subgroups","text":"","title":"Subgroups "},{"location":"about/about/#materials-project","text":"Donny Winston , Shyam Dwaraknath, Patrick Huck, John Dagdelen, David Mrdjenovich, Matthew Horton","title":"Materials Project:"},{"location":"about/about/#spectroscopy","text":"Shyam Dwaraknath , Handong Ling, Matthew McDermott","title":"Spectroscopy:"},{"location":"about/about/#materials-design-photocatalysts-magnetocalorics-piezoelectrics-tcos-etc","text":"Joey Montoya , Shyam Dwaraknath, Rachel Woods-Robinson, Matthew Horton, Jordan Burns, Handong Ling","title":"Materials Design (photocatalysts, magnetocalorics, piezoelectrics, TCOs, etc):"},{"location":"about/about/#electrodes","text":"Jimmy Shen , Eric Sivonxay, Jianli Cheng, Ann Rutt, Jordan Burns, Matthew Horton","title":"Electrodes:"},{"location":"about/about/#synthesis","text":"John Dagdelen , Rebecca Stern, David Mrdjenovich, Matthew McDermott, Matthew Horton","title":"Synthesis:"},{"location":"about/about/#electrolytes-sei","text":"Trevor Seguin , Brandon Wood, Julian Self, Tingzheng Hou, Sam Blau, Kara Fong","title":"Electrolytes-SEI:"},{"location":"about/about/#group-events","text":"Some of the regular things we do as a group are: Lunch every two weeks jointly with our group and members of the Jain research group. A group outing once every semester. Past activities included paintball and cardboard boat races in Kristin's pool. Twice-yearly \u201cCodeBusters\u201d events, where we spend 3 days hacking on code together. Dinners to celebrate group members who are moving on to join universities or companies. An annual joint holiday party with the Ceder Group.","title":"Group events"},{"location":"about/group_jobs/","text":"Persson Group Jobs \u2013 (Last updated Spring 2019) Job Name DFT Guru: Assist with general questions/errors related to DFT (especially VASP) Jimmy Shen Emmet Guru: Database builders; training of new people in builders Shyam Dwaraknath Group Chef: manage snacks/food at group meetings Jordan Burns Group Historian: Schedule and take group picture at least twice a year; document group outings/events and maintain these on the shared drive Oxana Andriuc Group Manager: Assign desk spaces, keep track of all laptops (LBL/Berkeley) and large assets with a DOE tag Rebecca Stern Group Webmaster: update the Persson Group web site with new projects, people, etc. (Donny assist) Sang-Won Park Coffee Tsar: Supply coffee for espresso machine Sang-Won Park Internal Group Wiki Manager I: Manage internal wiki on group basics, e.g. how to run specific calculations David Mrjdenovich Internal Group Wiki Manager II: Manage internal wiki on group basics, e.g. how to run specific calculations Kara Fong Job Master: Assign all group jobs, create and transfer jobs as needed, manage job transitions between new group members and alumni Matthew McDermott Molecular Explorer: QChem workflows and compiling support; programming challenge debugging Sam Blau MP Wiki Guru: update the Materials Project Wiki (in coordination with MP staff). Look for outdated information and make improvements as possible Tingzheng Hou NERSC Liaison: ERCAP allocations; VASP compilation on Savio Eric Sivonxay Onboarding Guide I: Assist new group members (esp. grad students) in getting started at LBL, update new member resources, e.g. the public group handbook Ann Rutt Onboarding Guide II: Assist new group members (esp. postdocs/staff) in getting started at LBL, update new member resources, e.g. the public group handbook Christian Legaspi Outreach Officer: Organize and disseminate videos from MP workshop; take notes with the goal of creating exportable content for the community Rachel Woods- Robinson HR: Lead setup of interview process for new postdocs Jianli Cheng PrinterMan: Outline directions for printing and update printer regularly with supplies Julian Self Production Gatekeeper: Assist with production jobs; VASP failures Patrick Huck Publications Manager: Assist Alice in publications for reports Xiaowei Xie Pymatgen Support: Group resource for questions related to pymatgen functionality, support for errors and bug fixes Matt Horton Seminar Master: Scheduling and managing group talks/seminars Trevor Seguin Social Chair I: Plan major group outings and retreat; assist in planning other socials Alex Epstein Social Chair II: Plan major group outings and retreat; assist in planning other socials Handong Ling Social Media Chair: Represent the group on Twitter, Facebook; assist in outreach John Dagdelen Web Guru: manage group web page; onboarding (programming side); group talks and posters archivist; group web bibliography Donny Winston","title":"Group Jobs"},{"location":"about/group_jobs/#persson-group-jobs","text":"(Last updated Spring 2019) Job Name DFT Guru: Assist with general questions/errors related to DFT (especially VASP) Jimmy Shen Emmet Guru: Database builders; training of new people in builders Shyam Dwaraknath Group Chef: manage snacks/food at group meetings Jordan Burns Group Historian: Schedule and take group picture at least twice a year; document group outings/events and maintain these on the shared drive Oxana Andriuc Group Manager: Assign desk spaces, keep track of all laptops (LBL/Berkeley) and large assets with a DOE tag Rebecca Stern Group Webmaster: update the Persson Group web site with new projects, people, etc. (Donny assist) Sang-Won Park Coffee Tsar: Supply coffee for espresso machine Sang-Won Park Internal Group Wiki Manager I: Manage internal wiki on group basics, e.g. how to run specific calculations David Mrjdenovich Internal Group Wiki Manager II: Manage internal wiki on group basics, e.g. how to run specific calculations Kara Fong Job Master: Assign all group jobs, create and transfer jobs as needed, manage job transitions between new group members and alumni Matthew McDermott Molecular Explorer: QChem workflows and compiling support; programming challenge debugging Sam Blau MP Wiki Guru: update the Materials Project Wiki (in coordination with MP staff). Look for outdated information and make improvements as possible Tingzheng Hou NERSC Liaison: ERCAP allocations; VASP compilation on Savio Eric Sivonxay Onboarding Guide I: Assist new group members (esp. grad students) in getting started at LBL, update new member resources, e.g. the public group handbook Ann Rutt Onboarding Guide II: Assist new group members (esp. postdocs/staff) in getting started at LBL, update new member resources, e.g. the public group handbook Christian Legaspi Outreach Officer: Organize and disseminate videos from MP workshop; take notes with the goal of creating exportable content for the community Rachel Woods- Robinson HR: Lead setup of interview process for new postdocs Jianli Cheng PrinterMan: Outline directions for printing and update printer regularly with supplies Julian Self Production Gatekeeper: Assist with production jobs; VASP failures Patrick Huck Publications Manager: Assist Alice in publications for reports Xiaowei Xie Pymatgen Support: Group resource for questions related to pymatgen functionality, support for errors and bug fixes Matt Horton Seminar Master: Scheduling and managing group talks/seminars Trevor Seguin Social Chair I: Plan major group outings and retreat; assist in planning other socials Alex Epstein Social Chair II: Plan major group outings and retreat; assist in planning other socials Handong Ling Social Media Chair: Represent the group on Twitter, Facebook; assist in outreach John Dagdelen Web Guru: manage group web page; onboarding (programming side); group talks and posters archivist; group web bibliography Donny Winston","title":"Persson Group Jobs \u2013"},{"location":"about/group_policies/","text":"Conference travel It is important to be connected to the research community. If it is your first year in the group, you can simply attend the conferences and listen to talks. After your first year, you are expected to be presenting talks or posters at conferences. This will ensure that: you keep up to date on developments in the field you will get to know the people in the field you are broadcasting your work to the research community. Many if not most people learn about new research by hearing about it at a conference. Thus, if you want people to know about your work, you must be willing to tell people about it. You should identify conferences you\u2019d like to attend several months (usually ~4 months, perhaps ~6 months for international travel) in advance. Usually, this is around the same time that abstract deadlines are due. Once you have identified a conference you\u2019d like to attend, please take the following actions: Tell Kristin about the conference and what project you\u2019d like to present As soon as possible - submit a conference travel request form. This form is a very basic (i.e., 2 minutes to fill out) Google spreadsheet: http://bit.ly/2niepv7 If you do not submit the travel request form several months in advance, you may not receive LBNL approval to attend. If you haven\u2019t done so already, make sure your travel profile (e.g., your frequent flier programs) are completed for the lab. E-mail esdradmin@lbl.gov if you don\u2019t have one yet. Work with Kristin to submit an abstract. You should send her the proposed abstract (with all details - title, authors, text, figures, etc.) with at least 3 days advance notice. Once you have received approval to attend the conference, please take the following steps: Make sure you register for the conference in time to receive any early registration discount (normally on one\u2019s own credit card then reimbursed later) Book a hotel (normally on one\u2019s own credit card then reimbursed later) Book a flight - please do this early to avoid last-minute flight rate spikes (normally booked in coordination with Tracee Tilman with LBNL making the booking. This works better if you identify desired flights in advance, otherwise give Tracee the preferred dates and times.) Note that if for any reason you book your own flights, you should be aware of various LBNL policies on flight booking such as preference for domestic carriers. If you are planning to combine vacation and travel, remember the lab\u2019s policy of taking only one vacation day per two work days. Note that days spent traveling to and from the conference count as work days. In terms of travel receipts and reimbursement: If you are traveling with funding through LBNL (i.e., most cases), you do not need to save receipts for meals. You will receive a per diem instead. You also do not need receipts for taxi rides under $75, although you may want to submits them anyway when you have them. You also do not need to save your actual airplane tickets for lab-purchased airfare, although again you may submit these anyway. If you are traveling with outside funding (e.g., the conference organizers are going to reimburse you), save all receipts and tickets as they may be needed for reimbursement. The proper way to request reimbursements for trips within the US is through the esdradmin site\u2019s \u201cTravel:Domestic\u201d tab available at http://bit.ly/2vGGWe9. If you have trouble, you can email esdradmin@lbl.gov. As for international trips including Canada, you should get in touch with the ESDR admin person that you work with. That person will provide you a corresponding form and help you through the (more complex) process of international-travel reimbursement. Pro tip: If you want to see the status of your conference requests, log in to this sheet with your LBNL account: http://bit.ly/2n6XCe3 You can filter the sheet to your requests by right-clicking on the name column and choosing the filter option. You should look for the \u201c(ADMINS ONLY) Approval status\u201d column in order to check your status.","title":"Group Policies"},{"location":"about/group_policies/#conference-travel","text":"It is important to be connected to the research community. If it is your first year in the group, you can simply attend the conferences and listen to talks. After your first year, you are expected to be presenting talks or posters at conferences. This will ensure that: you keep up to date on developments in the field you will get to know the people in the field you are broadcasting your work to the research community. Many if not most people learn about new research by hearing about it at a conference. Thus, if you want people to know about your work, you must be willing to tell people about it. You should identify conferences you\u2019d like to attend several months (usually ~4 months, perhaps ~6 months for international travel) in advance. Usually, this is around the same time that abstract deadlines are due. Once you have identified a conference you\u2019d like to attend, please take the following actions: Tell Kristin about the conference and what project you\u2019d like to present As soon as possible - submit a conference travel request form. This form is a very basic (i.e., 2 minutes to fill out) Google spreadsheet: http://bit.ly/2niepv7 If you do not submit the travel request form several months in advance, you may not receive LBNL approval to attend. If you haven\u2019t done so already, make sure your travel profile (e.g., your frequent flier programs) are completed for the lab. E-mail esdradmin@lbl.gov if you don\u2019t have one yet. Work with Kristin to submit an abstract. You should send her the proposed abstract (with all details - title, authors, text, figures, etc.) with at least 3 days advance notice. Once you have received approval to attend the conference, please take the following steps: Make sure you register for the conference in time to receive any early registration discount (normally on one\u2019s own credit card then reimbursed later) Book a hotel (normally on one\u2019s own credit card then reimbursed later) Book a flight - please do this early to avoid last-minute flight rate spikes (normally booked in coordination with Tracee Tilman with LBNL making the booking. This works better if you identify desired flights in advance, otherwise give Tracee the preferred dates and times.) Note that if for any reason you book your own flights, you should be aware of various LBNL policies on flight booking such as preference for domestic carriers. If you are planning to combine vacation and travel, remember the lab\u2019s policy of taking only one vacation day per two work days. Note that days spent traveling to and from the conference count as work days. In terms of travel receipts and reimbursement: If you are traveling with funding through LBNL (i.e., most cases), you do not need to save receipts for meals. You will receive a per diem instead. You also do not need receipts for taxi rides under $75, although you may want to submits them anyway when you have them. You also do not need to save your actual airplane tickets for lab-purchased airfare, although again you may submit these anyway. If you are traveling with outside funding (e.g., the conference organizers are going to reimburse you), save all receipts and tickets as they may be needed for reimbursement. The proper way to request reimbursements for trips within the US is through the esdradmin site\u2019s \u201cTravel:Domestic\u201d tab available at http://bit.ly/2vGGWe9. If you have trouble, you can email esdradmin@lbl.gov. As for international trips including Canada, you should get in touch with the ESDR admin person that you work with. That person will provide you a corresponding form and help you through the (more complex) process of international-travel reimbursement. Pro tip: If you want to see the status of your conference requests, log in to this sheet with your LBNL account: http://bit.ly/2n6XCe3 You can filter the sheet to your requests by right-clicking on the name column and choosing the filter option. You should look for the \u201c(ADMINS ONLY) Approval status\u201d column in order to check your status.","title":"Conference travel "},{"location":"computing/atomate/","text":"This guide walks you through the steps for setting up Fireworks and Atomate on the HPC. This guide will generally cover installation on both NERSC and Lawrencium. Installation steps for a local machine or another super computer will differ, but follow similar steps. Setup an environment: Create the conda environment Nersc module load python/3.6-anaconda-4.4 conda create -n cms python=3.6 Savio module load python/3.6 conda create -n cms python=3.6 Lawrencium module load python/3.6 conda create -n cms python=3.6 Activate the environment and install the base libraries Note: cms=computational materials science, but feel free to pick your own name source activate cms conda install numpy scipy matplotlib pandas to install atomate on the environment, use one of the following: If you are just using atomate and fireworks, with no plans to develop code/ workflows, use the following: pip install atomate For developers: source activate cms cd ~/.conda/envs/cms mkdir code cd code git clone https://github.com/materialsproject/pymatgen.git git clone https://github.com/materialsproject/pymatgen-db.git git clone https://github.com/materialsproject/fireworks.git git clone https://github.com/materialsproject/custodian.git git clone https://github.com/hackingmaterials/atomate.git cd pymatgen python setup.py develop pip install -r requirements.txt cd .. cd pymatgen-db python setup.py develop pip install -r requirements.txt cd .. cd fireworks python setup.py develop pip install -r requirements.txt cd .. cd custodian python setup.py develop pip install -r requirements.txt cd .. cd atomate python setup.py develop pip install -r requirements.txt cd .. Configure Fireworks: Create Fireworks configuration files Make directory in environment\u2019s root directory (/envs/<name>/) called config cd ~/.conda/envs/cms mkdir config cd config Make 3 files: FW_config.yaml, db.json, and my_launchpad.yaml with the following contents. Replace the teal highlighted text with details specific to your configuration. Note that you can view your filesystem online . Fireworks configuration (FW_config.yaml) Nersc CONFIG_FILE_DIR: /global/homes/s/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Savio CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Lawrencium CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Database file (db.json) { \"host\": \"mongodb03.nersc.gov\", \"port\": 27017, \"database\": \"fw_db_name\", \"collection\": \"tasks\", \"admin_password\": \"<admin_password>\", \"admin_user\": \"<admin_username>\", \"readonly_password\": \"<readonly_password>\", \"readonly_user\": \"<readonly_username>\", \"aliases_config\": null } Fireworks LaunchPad file (my_launchpad.yaml) host: mongodb03.nersc.gov port: 27017 name: 'fw_db_name' username: '<admin_username>' password: \u2018<admin_password>\u2019 logdir: null Istrm_lvl: DEBUG user_indices: [] wf_user_indices: [] Create Fireworker and Queue Launchers: Fireworker file (my_fworker.yaml) Nersc name: nersc_fworker category: '' query: '{}' env: db_file: /global/homes/s/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'srun -N 1 -n 64 -c 4 --cpu_bind=cores vasp_std' gamma_vasp_cmd: 'srun -N 1 -n 64 -c 4 --cpu_bind=cores vasp_gam' scratch_dir: /global/cscratch1/sd/sivonxay incar_update: Savio name: savio_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Lawrencium name: lrc_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Queue Adapter (my_qadapter.yaml): Nersc _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/homes/s/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: matgen job_name: knl_launcher qos: regular constraint: 'knl' pre_rocket: | source activate cms module load vasp-tpc/5.4.4-knl export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 Savio _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/home/users/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: co_lsdi job_name: knl_launcher queue: savio2_knl qos: lsdi_knl2_normal ntasks: 64 pre_rocket: | source activate cms module load vasp export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 post_rocket: null Lawrencium Available queues, partitions, and qos can be found at the following links: NERSC Savio Lawrencium Note: specifying singleshot in the queue adapter will limit each reserved job to running only one firework (even if other fireworks are ready and could run with your remaining wall time). Can change to rapidfire but this may result in lost runs (fireworks that do not complete because they run out of wall time). For more information on best practices for running VASP on multiple nodes (i.e. how to set vasp_cmd in my_fworker.yaml based on the number of nodes requested in my_qadapter) see the NERSC vasp training Configure Bash profile: Append the following lines to the .bashrc.ext file (which is located in your home directory, e.g. /global/homes/s/sivonxay) Nersc module load python/3.6-anaconda-4.4 export PMG_VASP_PSP_DIR=/project/projectdirs/matgen/POTCARs export FW_CONFIG_FILE='/global/homes/s/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Savio module load python/3.6 export PMG_VASP_PSP_DIR=/clusterfs/mp/software/POTCARs export FW_CONFIG_FILE='/global/home/users/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Lawrencium Note the line alias cdconfig='cd ~/.conda/envs/cms/config' is optional but is recommended so you can more easily locate the directory with your configuration files stored by typing cdconfig into the command line. Setup API key and Add POTCAR Directory to pymatgen Go to materialsproject.org and get an API key. Make sure you are using the environment setup above to run these commands. Nersc pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /project/projectdirs/matgen/POTCARs Savio pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /clusterfs/cloudcuckoo/POTCARs Running these commands will create a .pmgrc.yaml file (if it doesn\u2019t exist already) containing these configuration settings in your home directory Using Fireworks and Atomate: Initializing Fireworks Database: Use care when running this command. It will wipe all existing entries in your fireworks database in the fireworks, workflows, and launches collections. lpad reset Using Atomate to create preset workflow The following code blocks are python code that should be run in jupyter or from the terminal. Make sure you are using the environment setup above. The computer running the code should have access to mongodb03.nersc.gov; this can be disregarded when running directly on nersc or when connected to to the LBNL intranet. For computers outside of LBNL, a VPN will need to be used. Run the following python code by creating a file named \u201cmake_workflow.py\u201d: from pymatgen import MPRester from fireworks import LaunchPad from atomate.vasp.workflows.presets.core import wf_static from atomate.vasp import powerups mpr = MPRester() structure = mpr.get_structure_by_material_id(\u201cmp-145\u201d) wf = wf_static(structure) wf = powerups.add_common_powerups(wf, {\"scratch_dir\":\">>scratch_dir<<\"}) lp = LaunchPad.auto_load() lp.add_wf(wf) The previous python script added a static energy calculation for Hexagonal Lu to your job database. python \u201cmake_workflow.py To check on the job, run the command: lpad get_fws -s READY Running Jobs: We recommend using a tmux or screen when submitting jobs to preserve queue submission when the main ssh session is terminated. This allows one to keep the queue saturated with jobs. For example, for tmux: module load tmux tmux new -s background_launcher source active cms mkdir FireworksTest cd FireworksTest To exit the tmux session, press the \u201ccontrol\u201d and \u201cb\u201d keys together, followed by \u201cd\u201d To re-enter the tmux session: tmux attach -t background_launcher For more tmux commands, see the tmux cheatsheet Use qlaunch command to reserve jobs with SLURM\u2019s scheduler. Qlaunch has 3 modes; singleshot, rapidfire, and multi. Singleshot is used to launch one job, rapidfire is used to launch multiple jobs in quick succession, and multi creates one job with multiple fireworks runs. You\u2019ll probably want to use rapidfire. Some useful flags to set are: -m to specify maximum # of jobs in queue at any given time and --nlaunches to specify how many fireworks to run. Here are some examples: Example_1 qlaunch singleshot Example_2 qlaunch rapidfire --nlaunches 10 Example_3 qlaunch rapidfire --nlaunches infinite -m 50 Example_4 qlaunch -fm rapidfire --nlaunches infinite -m 50 Monitoring Database/ Fireworks Status of running fireworks can be determined by using the command Example_1 lpad get_fws -s RUNNING Example_2 lpad get_fws -q '{\"state\":\"RUNNING\", \"fw_id\":{\"$gte\": 50}}' Rerunning fizzled (Failed) fireworks Example_1 lpad rerun_fws -i <fw_id> Example_2 lpad rerun_fws -s FIZZLED Alternatively, you can view your jobs in the GUI by downloading your my_launchpad.yaml file to your local machine. To do so: scp <username>@dtn01.nersc.gov:~/.conda/envs/cms/config/my_launchpad.yaml . Then, run this command, locally, in the folder containing the my_launchpad.yaml file. This will bring up the web interface hosted on you local machine on port 5000 (127.0.0.1:5000). lpad webgui Occasionally, jobs may hang and not complete. To check for hung jobs, use the command: lpad detect_lostruns To rerun these jobs, add the --rerun flag to the command. lpad detect_lostruns --rerun Viewing Database and Outputs: Download and install Robo 3T from the following link Setup connection using mongodb credentials Querying collections Authors: Eric Sivonxay, Julian Self, Ann Rutt, and Oxana Andriuc Contact: esivonxay@lbl.gov and jself@lbl.gov","title":"Atomate Setup"},{"location":"computing/atomate/#setup-an-environment","text":"Create the conda environment Nersc module load python/3.6-anaconda-4.4 conda create -n cms python=3.6 Savio module load python/3.6 conda create -n cms python=3.6 Lawrencium module load python/3.6 conda create -n cms python=3.6 Activate the environment and install the base libraries Note: cms=computational materials science, but feel free to pick your own name source activate cms conda install numpy scipy matplotlib pandas to install atomate on the environment, use one of the following: If you are just using atomate and fireworks, with no plans to develop code/ workflows, use the following: pip install atomate For developers: source activate cms cd ~/.conda/envs/cms mkdir code cd code git clone https://github.com/materialsproject/pymatgen.git git clone https://github.com/materialsproject/pymatgen-db.git git clone https://github.com/materialsproject/fireworks.git git clone https://github.com/materialsproject/custodian.git git clone https://github.com/hackingmaterials/atomate.git cd pymatgen python setup.py develop pip install -r requirements.txt cd .. cd pymatgen-db python setup.py develop pip install -r requirements.txt cd .. cd fireworks python setup.py develop pip install -r requirements.txt cd .. cd custodian python setup.py develop pip install -r requirements.txt cd .. cd atomate python setup.py develop pip install -r requirements.txt cd ..","title":"Setup an environment:"},{"location":"computing/atomate/#configure-fireworks","text":"","title":"Configure Fireworks:"},{"location":"computing/atomate/#create-fireworks-configuration-files","text":"Make directory in environment\u2019s root directory (/envs/<name>/) called config cd ~/.conda/envs/cms mkdir config cd config Make 3 files: FW_config.yaml, db.json, and my_launchpad.yaml with the following contents. Replace the teal highlighted text with details specific to your configuration. Note that you can view your filesystem online .","title":"Create Fireworks configuration files"},{"location":"computing/atomate/#fireworks-configuration-fw_configyaml","text":"Nersc CONFIG_FILE_DIR: /global/homes/s/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Savio CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5 Lawrencium CONFIG_FILE_DIR: /global/home/users/sivonxay/.conda/envs/cms/config QUEUE_UPDATE_INTERVAL: 5","title":"Fireworks configuration (FW_config.yaml)"},{"location":"computing/atomate/#database-file-dbjson","text":"{ \"host\": \"mongodb03.nersc.gov\", \"port\": 27017, \"database\": \"fw_db_name\", \"collection\": \"tasks\", \"admin_password\": \"<admin_password>\", \"admin_user\": \"<admin_username>\", \"readonly_password\": \"<readonly_password>\", \"readonly_user\": \"<readonly_username>\", \"aliases_config\": null }","title":"Database file (db.json)"},{"location":"computing/atomate/#fireworks-launchpad-file-my_launchpadyaml","text":"host: mongodb03.nersc.gov port: 27017 name: 'fw_db_name' username: '<admin_username>' password: \u2018<admin_password>\u2019 logdir: null Istrm_lvl: DEBUG user_indices: [] wf_user_indices: []","title":"Fireworks LaunchPad file (my_launchpad.yaml)"},{"location":"computing/atomate/#create-fireworker-and-queue-launchers","text":"","title":"Create Fireworker and Queue Launchers:"},{"location":"computing/atomate/#fireworker-file-my_fworkeryaml","text":"Nersc name: nersc_fworker category: '' query: '{}' env: db_file: /global/homes/s/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'srun -N 1 -n 64 -c 4 --cpu_bind=cores vasp_std' gamma_vasp_cmd: 'srun -N 1 -n 64 -c 4 --cpu_bind=cores vasp_gam' scratch_dir: /global/cscratch1/sd/sivonxay incar_update: Savio name: savio_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update: Lawrencium name: lrc_fworker category: '' query: '{}' env: db_file: /global/home/users/sivonxay/.conda/envs/cms/config/db.json vasp_cmd: 'mpirun --bind-to core vasp_std' gamma_vasp_cmd: 'mpirun --bind-to core vasp_gam' scratch_dir: /global/scratch/sivonxay/ incar_update:","title":"Fireworker file (my_fworker.yaml)"},{"location":"computing/atomate/#queue-adapter-my_qadapteryaml","text":"Nersc _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/homes/s/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: matgen job_name: knl_launcher qos: regular constraint: 'knl' pre_rocket: | source activate cms module load vasp-tpc/5.4.4-knl export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 Savio _fw_name: CommonAdapter _fw_q_type: SLURM rocket_launch: rlaunch -w /global/home/users/sivonxay/.conda/envs/cms/config/my_fworker.yaml singleshot nodes: 1 walltime: '24:00:00' account: co_lsdi job_name: knl_launcher queue: savio2_knl qos: lsdi_knl2_normal ntasks: 64 pre_rocket: | source activate cms module load vasp export OMP_PROC_BIND=true export OMP_PLACES=threads export OMP_NUM_THREADS=1 post_rocket: null Lawrencium Available queues, partitions, and qos can be found at the following links: NERSC Savio Lawrencium Note: specifying singleshot in the queue adapter will limit each reserved job to running only one firework (even if other fireworks are ready and could run with your remaining wall time). Can change to rapidfire but this may result in lost runs (fireworks that do not complete because they run out of wall time). For more information on best practices for running VASP on multiple nodes (i.e. how to set vasp_cmd in my_fworker.yaml based on the number of nodes requested in my_qadapter) see the NERSC vasp training","title":"Queue Adapter (my_qadapter.yaml):"},{"location":"computing/atomate/#configure-bash-profile","text":"Append the following lines to the .bashrc.ext file (which is located in your home directory, e.g. /global/homes/s/sivonxay) Nersc module load python/3.6-anaconda-4.4 export PMG_VASP_PSP_DIR=/project/projectdirs/matgen/POTCARs export FW_CONFIG_FILE='/global/homes/s/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Savio module load python/3.6 export PMG_VASP_PSP_DIR=/clusterfs/mp/software/POTCARs export FW_CONFIG_FILE='/global/home/users/sivonxay/.conda/envs/cms/config/FW_config.yaml' alias cdconfig='cd ~/.conda/envs/cms/config' Lawrencium Note the line alias cdconfig='cd ~/.conda/envs/cms/config' is optional but is recommended so you can more easily locate the directory with your configuration files stored by typing cdconfig into the command line.","title":"Configure Bash profile:"},{"location":"computing/atomate/#setup-api-key-and-add-potcar-directory-to-pymatgen","text":"Go to materialsproject.org and get an API key. Make sure you are using the environment setup above to run these commands. Nersc pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /project/projectdirs/matgen/POTCARs Savio pmg config --add PMG_MAPI_KEY <USER_API_KEY> pmg config --add PMG_VASP_PSP_DIR /clusterfs/cloudcuckoo/POTCARs Running these commands will create a .pmgrc.yaml file (if it doesn\u2019t exist already) containing these configuration settings in your home directory","title":"Setup API key and Add POTCAR Directory to pymatgen"},{"location":"computing/atomate/#using-fireworks-and-atomate","text":"","title":"Using Fireworks and Atomate:"},{"location":"computing/atomate/#initializing-fireworks-database","text":"Use care when running this command. It will wipe all existing entries in your fireworks database in the fireworks, workflows, and launches collections. lpad reset","title":"Initializing Fireworks Database:"},{"location":"computing/atomate/#using-atomate-to-create-preset-workflow","text":"The following code blocks are python code that should be run in jupyter or from the terminal. Make sure you are using the environment setup above. The computer running the code should have access to mongodb03.nersc.gov; this can be disregarded when running directly on nersc or when connected to to the LBNL intranet. For computers outside of LBNL, a VPN will need to be used. Run the following python code by creating a file named \u201cmake_workflow.py\u201d: from pymatgen import MPRester from fireworks import LaunchPad from atomate.vasp.workflows.presets.core import wf_static from atomate.vasp import powerups mpr = MPRester() structure = mpr.get_structure_by_material_id(\u201cmp-145\u201d) wf = wf_static(structure) wf = powerups.add_common_powerups(wf, {\"scratch_dir\":\">>scratch_dir<<\"}) lp = LaunchPad.auto_load() lp.add_wf(wf) The previous python script added a static energy calculation for Hexagonal Lu to your job database. python \u201cmake_workflow.py To check on the job, run the command: lpad get_fws -s READY","title":"Using Atomate to create preset workflow"},{"location":"computing/atomate/#running-jobs","text":"We recommend using a tmux or screen when submitting jobs to preserve queue submission when the main ssh session is terminated. This allows one to keep the queue saturated with jobs. For example, for tmux: module load tmux tmux new -s background_launcher source active cms mkdir FireworksTest cd FireworksTest To exit the tmux session, press the \u201ccontrol\u201d and \u201cb\u201d keys together, followed by \u201cd\u201d To re-enter the tmux session: tmux attach -t background_launcher For more tmux commands, see the tmux cheatsheet Use qlaunch command to reserve jobs with SLURM\u2019s scheduler. Qlaunch has 3 modes; singleshot, rapidfire, and multi. Singleshot is used to launch one job, rapidfire is used to launch multiple jobs in quick succession, and multi creates one job with multiple fireworks runs. You\u2019ll probably want to use rapidfire. Some useful flags to set are: -m to specify maximum # of jobs in queue at any given time and --nlaunches to specify how many fireworks to run. Here are some examples: Example_1 qlaunch singleshot Example_2 qlaunch rapidfire --nlaunches 10 Example_3 qlaunch rapidfire --nlaunches infinite -m 50 Example_4 qlaunch -fm rapidfire --nlaunches infinite -m 50","title":"Running Jobs:"},{"location":"computing/atomate/#monitoring-database-fireworks","text":"Status of running fireworks can be determined by using the command Example_1 lpad get_fws -s RUNNING Example_2 lpad get_fws -q '{\"state\":\"RUNNING\", \"fw_id\":{\"$gte\": 50}}' Rerunning fizzled (Failed) fireworks Example_1 lpad rerun_fws -i <fw_id> Example_2 lpad rerun_fws -s FIZZLED Alternatively, you can view your jobs in the GUI by downloading your my_launchpad.yaml file to your local machine. To do so: scp <username>@dtn01.nersc.gov:~/.conda/envs/cms/config/my_launchpad.yaml . Then, run this command, locally, in the folder containing the my_launchpad.yaml file. This will bring up the web interface hosted on you local machine on port 5000 (127.0.0.1:5000). lpad webgui Occasionally, jobs may hang and not complete. To check for hung jobs, use the command: lpad detect_lostruns To rerun these jobs, add the --rerun flag to the command. lpad detect_lostruns --rerun","title":"Monitoring Database/ Fireworks"},{"location":"computing/atomate/#viewing-database-and-outputs","text":"Download and install Robo 3T from the following link","title":"Viewing Database and Outputs:"},{"location":"computing/atomate/#setup-connection-using-mongodb-credentials","text":"","title":"Setup connection using mongodb credentials"},{"location":"computing/atomate/#querying-collections","text":"Authors: Eric Sivonxay, Julian Self, Ann Rutt, and Oxana Andriuc Contact: esivonxay@lbl.gov and jself@lbl.gov","title":"Querying collections"},{"location":"computing/calculation_software/","text":"VASP We generally run 2 versions of VASP - 5.4.4 and 6 (in beta). VASP 6 is used primarily on nodes operating on the Knights Landing (KNL) CPU architecture, while VASP 5.4.4 is generally used on all other CPUs. VASP on NERSC NERSC maintains compiled, and optimized versions of the vasp binaries. To get access to these binaries, you must be added to the group VASP license. If/when you determine that your research requires the use of VASP, contact Jimmy Shen and Eric Sivonxay. The potcar directory is: /project/projectdirs/matgen/POTCARs VASP on Berkeley Research Computing (savio) When running vasp on savio, we have access to our own compilations. To access the vasp binaries on savio, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles POTCARs are located at: /clusterfs/cloudcuckoo/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov) Lawrencium When running vasp on Lawrencium, we have access to our own compilations. To access the vasp binaries on Lawrencium, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/clusterfs/mp/temp_modfiles POTCARs are located at: /clusterfs/mp/software/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov) LAMMPS GROMACS Authors: Eric Sivonxay Contact: esivonxay@lbl.gov","title":"Calculation Software"},{"location":"computing/calculation_software/#vasp","text":"We generally run 2 versions of VASP - 5.4.4 and 6 (in beta). VASP 6 is used primarily on nodes operating on the Knights Landing (KNL) CPU architecture, while VASP 5.4.4 is generally used on all other CPUs.","title":"VASP"},{"location":"computing/calculation_software/#vasp-on-nersc","text":"NERSC maintains compiled, and optimized versions of the vasp binaries. To get access to these binaries, you must be added to the group VASP license. If/when you determine that your research requires the use of VASP, contact Jimmy Shen and Eric Sivonxay. The potcar directory is: /project/projectdirs/matgen/POTCARs","title":"VASP on NERSC"},{"location":"computing/calculation_software/#vasp-on-berkeley-research-computing-savio","text":"When running vasp on savio, we have access to our own compilations. To access the vasp binaries on savio, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles POTCARs are located at: /clusterfs/cloudcuckoo/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov)","title":"VASP on Berkeley Research Computing (savio)"},{"location":"computing/calculation_software/#lawrencium","text":"When running vasp on Lawrencium, we have access to our own compilations. To access the vasp binaries on Lawrencium, add the following line to your .bashrc file. export MODULEPATH=${MODULEPATH}:/clusterfs/mp/temp_modfiles POTCARs are located at: /clusterfs/mp/software/POTCARs For issues with these compilations, please contact Eric Sivonxay (esivonxay@lbl.gov)","title":"Lawrencium"},{"location":"computing/calculation_software/#lammps","text":"","title":"LAMMPS"},{"location":"computing/calculation_software/#gromacs","text":"Authors: Eric Sivonxay Contact: esivonxay@lbl.gov","title":"GROMACS"},{"location":"computing/computing/","text":"Setting up a new Macbook Upgrade your OS If your computer is not using the latest OS, you should upgrade to the latest OS first. Installing Python development environment The best way to manage Python installations these days is a \u201cconda env\u201d. This will allow you to manage different Python \u201cenvironments\u201d, where each environment is a set of libraries that you have installed. For example, you can have one environment that uses Python 2.7 and has certain library versions installed, and another environment that uses Python 3.5 and has other libraries installed. Another advantage of conda environments is that you can apply the same procedure on NERSC and other computing centers that support conda. To do this, follow the online instructions on installing a conda environment and see modifications below: * http://conda.pydata.org/docs/using/index.html * (probably) prefer to install the \u201cminiconda\u201d version rather than anaconda * (probably) prefer to install \u201cminiconda 3\u201d rather than \u201cminiconda 2\u201d. Both will work fine and allow you to do everything the other one does so don\u2019t stress too much about this decision. * When creating environments, use a command like this (note that this also installs recommended libraries): conda create --name py3 python=3 numpy matplotlib seaborn plotly pandas flask pymongo scipy sympy scikit-learn jupyter If you want a reference guide to conda commands, try: http://conda.pydata.org/docs/using/cheatsheet.html Install high-throughput computation environment Our group has a set of base codebases used for performing high-throughput calculations. Note that if your project does not involve high-throughput calculation, you may need only one or two of these libraries installed \u2013 ask your subgroup head if you are unsure. After activating a conda environment , install the following packages using a combination of git clone >>REPO_NAME<< and python setup.py develop. Start with: git clone https://www.github.com/materialsproject/fireworks You might need to generate an ssh key for the git clone command to work: ssh-keygen -t rsa -b 4096 no password is probably OK unless you are security conscious add your SSH key to your Github profile Then: cd fireworks; python setup.py develop; cd .. Repeat the process above for the remaining libraries: git clone https://www.github.com/materialsproject/pymatgen cd pymatgen; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/pymatgen-db cd pymatgen-db; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/custodian cd custodian; python setup.py develop; cd .. Repeat the same process for a couple of other libraries on the hackingmaterials github site: git clone https://www.github.com/hackingmaterials/atomate cd atomate; python setup.py develop; cd .. git clone https://www.github.com/hackingmaterials/matminer cd matminer; python setup.py develop; cd .. If you want, you can automatically source activate your environment in your .bash_profile file. This will automatically load your environment when you open a Terminal. Otherwise, you will start off in your default Mac Python. Other things to do Set up your Time Machine backup (make sure you have purchased or received an external hard disk). https://support.apple.com/en-us/HT204412 You can also set up an online backup plan (e.g., Crashplan or Backblaze) to provide you with a second backup. Install MongoDB. Install Docker. Purchase Microsoft office from LBNL software distribution. Printing and Scanning The group has a Canon MF731C printer located in 33-143C. It supports color printing as well as two-sided printing and includes a scanner. To connect to the printer follow this guide on the internal group site . Software help groups If you have problems with software, and in particular the software maintained by our group and our collaborators, you should contact the appropriate help group. The documentation for the software will list what that channel is; if not, try the Github Issues page. If you are reaching out for help, try to provide everything needed to quickly reproduce and debug the problem (files, test code, etc). Two other ways to get software help that are more self-guided are: If you are having trouble using a particular class or function, look for unit tests within the code, which often demonstrate how to use the class or function If the class or function has a unique name (e.g., MaterialsProjectCompatibility), another option is to both Google and search on github.com for the particular class/function. The github.com search will often reveal code snippets from users all around the world. Authors: Kara Fong Contact: karafong@lbl.gov","title":"General Info"},{"location":"computing/computing/#setting-up-a-new-macbook","text":"","title":"Setting up a new Macbook"},{"location":"computing/computing/#upgrade-your-os","text":"If your computer is not using the latest OS, you should upgrade to the latest OS first.","title":"Upgrade your OS"},{"location":"computing/computing/#installing-python-development-environment","text":"The best way to manage Python installations these days is a \u201cconda env\u201d. This will allow you to manage different Python \u201cenvironments\u201d, where each environment is a set of libraries that you have installed. For example, you can have one environment that uses Python 2.7 and has certain library versions installed, and another environment that uses Python 3.5 and has other libraries installed. Another advantage of conda environments is that you can apply the same procedure on NERSC and other computing centers that support conda. To do this, follow the online instructions on installing a conda environment and see modifications below: * http://conda.pydata.org/docs/using/index.html * (probably) prefer to install the \u201cminiconda\u201d version rather than anaconda * (probably) prefer to install \u201cminiconda 3\u201d rather than \u201cminiconda 2\u201d. Both will work fine and allow you to do everything the other one does so don\u2019t stress too much about this decision. * When creating environments, use a command like this (note that this also installs recommended libraries): conda create --name py3 python=3 numpy matplotlib seaborn plotly pandas flask pymongo scipy sympy scikit-learn jupyter If you want a reference guide to conda commands, try: http://conda.pydata.org/docs/using/cheatsheet.html","title":"Installing Python development environment"},{"location":"computing/computing/#install-high-throughput-computation-environment","text":"Our group has a set of base codebases used for performing high-throughput calculations. Note that if your project does not involve high-throughput calculation, you may need only one or two of these libraries installed \u2013 ask your subgroup head if you are unsure. After activating a conda environment , install the following packages using a combination of git clone >>REPO_NAME<< and python setup.py develop. Start with: git clone https://www.github.com/materialsproject/fireworks You might need to generate an ssh key for the git clone command to work: ssh-keygen -t rsa -b 4096 no password is probably OK unless you are security conscious add your SSH key to your Github profile Then: cd fireworks; python setup.py develop; cd .. Repeat the process above for the remaining libraries: git clone https://www.github.com/materialsproject/pymatgen cd pymatgen; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/pymatgen-db cd pymatgen-db; python setup.py develop; cd .. git clone https://www.github.com/materialsproject/custodian cd custodian; python setup.py develop; cd .. Repeat the same process for a couple of other libraries on the hackingmaterials github site: git clone https://www.github.com/hackingmaterials/atomate cd atomate; python setup.py develop; cd .. git clone https://www.github.com/hackingmaterials/matminer cd matminer; python setup.py develop; cd .. If you want, you can automatically source activate your environment in your .bash_profile file. This will automatically load your environment when you open a Terminal. Otherwise, you will start off in your default Mac Python.","title":"Install high-throughput computation environment"},{"location":"computing/computing/#other-things-to-do","text":"Set up your Time Machine backup (make sure you have purchased or received an external hard disk). https://support.apple.com/en-us/HT204412 You can also set up an online backup plan (e.g., Crashplan or Backblaze) to provide you with a second backup. Install MongoDB. Install Docker. Purchase Microsoft office from LBNL software distribution.","title":"Other things to do"},{"location":"computing/computing/#printing-and-scanning","text":"The group has a Canon MF731C printer located in 33-143C. It supports color printing as well as two-sided printing and includes a scanner. To connect to the printer follow this guide on the internal group site .","title":"Printing and Scanning "},{"location":"computing/computing/#software-help-groups","text":"If you have problems with software, and in particular the software maintained by our group and our collaborators, you should contact the appropriate help group. The documentation for the software will list what that channel is; if not, try the Github Issues page. If you are reaching out for help, try to provide everything needed to quickly reproduce and debug the problem (files, test code, etc). Two other ways to get software help that are more self-guided are: If you are having trouble using a particular class or function, look for unit tests within the code, which often demonstrate how to use the class or function If the class or function has a unique name (e.g., MaterialsProjectCompatibility), another option is to both Google and search on github.com for the particular class/function. The github.com search will often reveal code snippets from users all around the world. Authors: Kara Fong Contact: karafong@lbl.gov","title":"Software help groups "},{"location":"computing/hpc/","text":"Computing Resources Our group\u2019s main computing resources are: NERSC (the LBNL supercomputing center, one of the biggest in the world) Lawrencium / Berkeley Research Computing Peregrine (the NREL supercomputing center) Argonne Leadership Computing Facility (sometimes) Oak Ridge Leadership Computing Facility (sometimes) At any time, if you feel you are computing-limited, please contact Kristin so she can work with you on finding solutions. NERSC To get started with calculations at NERSC: Ask Kristin about whether you will be running at NERSC and, if so, under what account / repository to charge. Request a NERSC account through the NERSC homepage (Google \u201cNERSC account request\u201d). A NERSC Liason or PI Proxy will validate your account and assign you an initial allocation of computing hours. At this point, you should be able to log in, check CPU-hour balances, etc. through \u201cNERSC NIM\u201d and \u201cMy NERSC\u201d portals In order to log in and run jobs on the various machines at NERSC, review the NERSC documentation. In order to load and submit scripts for various codes (VASP, ABINIT, Quantum Espresso), NERSC has lots of information to help. Try Google, e.g. \u201cNERSC VASP\u201d. ... * Note that for commercial codes such as VASP, there is an online form that allows you to enter your VASP license, which NERSC will confirm and then allow you access to. Please make a folder inside your project directory and submit all your jobs there, as your home folder has only about 40GB of space. For example, for matgen project, your work folder path should be something like the following: /global/project/projectdirs/matgen/YOUR_NERSC_USERNAME You can also request a mongo database for your project to be hosted on NERSC. Google \u201cMongoDB on NERSC\u201d for instructions. Donny Winston or Patrick Huck can also help you get set up and provide you with a preconfigured database suited for running Materials Project style workflows. Running Jobs on NERSC This tutorial provides a brief overview of setting yourself up to run jobs on NERSC. If any information is unclear or missing, feel free to edit this document or contact Kara Fong. Setting up a NERSC account: Contact the group\u2019s NERSC Liaison (currently Eric Sivonxay, see Group Jobs list). They will help you create an account and allocate you computational resources. You will then receive an email with instructions to fill out the Appropriate Use Policy form, set up your password, etc. Once your account is set up, you can manage it at the NERSC Information Management (NIM) website. Connecting with SSH: You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). Run the command ssh-keygen -t rsa -b 4096. This will generate an RSA key, which you can view in the file id_rsa.pub You\u2019ll be asked to enter a passphrase, which should be different from your password. You must store your SSH public key on the NERSC NIM database. Go to the NIM website, navigate to \u201cMy Stuff\u201d -> \u201cMy SSH Keys\u201d. Click on the SSH Keys tab. Copy your key (from id_rsa.pub) into the website\u2019s text box, click Add. Logging on: Log on to Cori, for example, by submitting the following command in the terminal: ssh username@cori.nersc.gov You will be prompted to enter your passphrase. This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias cori=\"ssh your_username@cori.nersc.gov\" Now you will be able to initialize a SSH connection to cori just by typing cori in the command line and pressing enter. Transferring files to/from NERSC: For small files, you can use SCP (secure copy). To get a file from NERSC, use: scp user_name@dtn01.nersc.gov:/remote/path/myfile.txt /local/path To send a file to NERSC, use: scp /local/path/myfile.txt user_name@dtn01.nersc.gov:/remote/path To move a larger quantity of data using a friendlier interface, use Globus Online. Running and monitoring jobs: The following instructions are for running on Cori. Analogous information for running on Edison can be found here. Most jobs are run in batch mode, in which you prepare a shell script telling the batch system how to run the job (number of nodes, time the job will run, etc.). NERSC\u2019s batch system software is called SLURM. Below is a simple batch script example, copied from the NERSC website: #!bin/bash -l #SBATCH -N 2 #Use 2 nodes #SBATCH -t 00:30:00 #Set 30 minute time limit #SBATCH -q regular #Submit to the regular QOS #SBATCH -L scratch #Job requires $SCRATCH file system #SBATCH -C haswell #Use Haswell nodes srun -n 32 -c 4 ./my_executable Here, the first line specifies which shell to use (in this case bash). The keyword #SBATCH is used to start directive lines ( click here for a full description of the sbatch options you can specify). The word \u201csrun\u201d starts execution of the code. To submit your batch script, use sbatch myscript.sl in the directory containing the script file. Below are some useful commands to control and monitor your jobs: sqs -u username (Lists jobs for your account) scancel jobid (Cancels a job from the queue) Choosing a QOS (quality of service): You specify which queue to use in your batch file. Use the debug queue for small, short test runs, the regular queue for production runs, and the premium queue for high-priority jobs. Choosing a node type (haswell vs knl): You may also specify the resource type you would like your job to run on, witnin your batch file. When running on Cori, there are two CPU architectures available, Haswell, and Knights Landing (known as KNL). Running on a Haswell node will afford you high individual core performance with up to 32 cores per node (or 64 threads per node). A KNL node provides a large core count (68 cores per node or 272 threads per node) which is suitable for programs capable of effectively utilizing multithreading. On Cori, there are 2388 Haswell nodes and 9688 KNL nodes. [TODO: FILL OUT THIS SECTION MORE] Automatic job submission on NERSC: crontab In order to automatically manage job submission at NERSC, you can use crontab. You can submit jobs periodically even when you are not signed in to any NERSC systems and perhaps reduce the queue time from 5-10 days to a few hours. This is possible because of the way jobs are managed in atomate/fireworks. Please make sure you feel comfortable submitting individual jobs via atomate before reading this section. In atomate, by using --maxloop 3 for example when setting rocket_launch in your my_qadapter.yaml, after 3 trials in each minute if there are still no READY jobs available in your Launchpad Fireworks would stop the running job on NERSC to avoid wasting computing resources. On the other hand, if you have Fireworks available with the READY state and you have been using crontab for a few days, even if the jobs you submitted a few days ago start running on NERSC, they would pull any READY Fireworks and start RUNNING them reducing the turnaround from a few days to a few hours! So how to setup crontab? Please follow the instructions here: 1. ssh to the node where you want to setup the crontab; try one that is easy to remember such as cori01 or edison01; for logging in to a specific node just do for example \u201cssh cori01\u201d after you log in to the system (Cori in this example). Type and enter: crontab -e Now you can setup the following command in the opened vi editor. What it does is basically running the SCRIPT.sh file every 120 minutes of every day of every week of every month of every year (or simply /120 * * *): */120 * * * * /bin/bash -l PATH_TO_SCRIPT.sh >> PATH_TO_LOGFILE Setup your SCRIPT.sh like the following: (as a suggestion, you can simply put this file and the log file which keeps a log of submission states in your home folder): source activate YOUR_PRODUCTION_CONDA_ENVIRONMENT FW_CONFIG_FILE=PATH_TO_CONFIG_DIR/FW_config.yaml cd PATH_TO_YOUR_PRODUCTION_FOLDER qlaunch --fill_mode rapidfire -m 1000 --nlaunches 1 The last line of this 3-line file is really what submitting your job inside your production folder with the settings that you set in FW_config.yaml file. See atomate documentation for more info. Please make sure to set your PRODUCTION_FOLDER under /global/project/projectdirs/ that has much more space than your home folder and it is also backed up. Make sure to keep an eye on how close you are to disk space and file number limitations by checking https://my.nersc.gov/ periodically. Running Jupyter Notebooks on Cori Jupyter notebooks are quickly becoming an indispensable tool for doing computational science. In some cases, you might want to (or need to) harness NERSC computing power inside of a jupyter notebook. To do this, you can use NERSC\u2019s new Jupyterhub system at https://jupyter-dev.nersc.gov/. These notebooks are run on a large memory node of Cori and can also submit jobs to the batch queues (see http://bit.ly/2A0mqrl for details). All of your files and the project directory will be accessible from the Jupyterhub, but your conda envs won\u2019t be available before you do some configuration. To set up a conda environment so it is accessible from the Jupyterhub, activate the environment and setup an ipython kernel. To do this, run the command \u201cpip install ipykernel\u201d. More info can be found at http://bit.ly/2yoKAzB. Automatic Job Packing with FireWorks DISCLAIMER: Only use job packing if you have trouble with typical job submission. The following tip is not 100% guaranteed to work., and is based on limited, subjective experience on Cori. Talk to Alex Dunn (ardunn@lbl.gov) for help if you have trouble. The Cori queue system can be unreasonably slow when submitting many (e.g., hundreds, thousands) of small (e.g., single node or 2 nodes) jobs with qos-normal priority on Haswell. In practice, we have found that the Cori job scheduler will give your jobs low throughput if you have many jobs in queue, and you will often only be able to run 5-30 jobs at a time, while the rest wait in queue for far longer than originally expected (e.g., weeks). While there is no easy way to increase your queue submission rate (AFAIK), you can use FireWorks job-packing to \u201ctrick\u201d Cori\u2019s SLURM scheduler into running many jobs in serial on many parallel compute nodes with a single queue submission, vastly increasing throughput. You can use job packing with the \u201cmulti\u201d option to rlaunch. This command launches N parallel python processes on the Cori scheduling node, each which runs a job using M compute nodes. The steps to job packing are: 1. Edit your my_qadapter.yaml file to reserve N * M nodes for each submission. For example, if each of your jobs takes M = 2 nodes, and you want a N = 10 x speedup, reserve 20 nodes per queue submission. 2. Change your rlaunch command to: rlaunch -c /your/config multi N To have each FireWorks process run as many jobs as possible in serial before the walltime, use the --nlaunches 0 option. To prevent FireWorks from submitting jobs with little walltime left (causing jobs to frequently get stuck as \u201cRUNNING\u201d), set the --timeout option. Make sure --timeout is set so that even a long running job submitted at the end of your allocation will not run over your walltime limit. Your my_qadapter.yaml should then have something similar to the following lines: rocket_launch: rlaunch -c /your/config multi 10 --nlaunches 0 --timeout 169200 nodes: 20 Typically, setting N <= 10 will give you a good N-times speedup with no problems. There are no guarantees, however, when N > 10-20. Use N > 50 at your own risk! Berkeley Research Computing Berkeley Research Computing (BRC) hosts the Savio supercomputing cluster. Savio operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. Faculty Computing Allowance (FCA) - Limited computing time provided to each Faculty member using Savio. Setting up a BRC account To get an account on Savio, fill out the form linked below, making sure to select the appropriate allocation. Typically, most students and postdocs will be running on co_lsdi. http://research-it.berkeley.edu/services/high-performance-computing/getting-account After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. [TODO: Fill out this with specific 2-factor setup details] Logging into BRC To access your shiny new savio account, you'll want to SSH onto the system from a terminal. ssh username@hpc.brc.berekeley.edu You will be prompted to enter your passphrase. This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias savio=\"ssh username@hpc.brc.berekeley.edu\" Running on BRC Under the condo account co_lsdi, we have exclusive access to 28 KNL nodes. Additionally, we have the ability to run on other nodes at low priority mode. Accessing Software binaries Software within BRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge Accessing In-House software packages The Persson Group maintains their own versions of popular codes such as VASP, GAUSSIAN, QCHEM and LAMMPS. To access these binaries, ensure that you have the proper licenses and permissions, then append the following line to the .bashrc file in your root directory: export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles Using Persson Group Owned KNL nodes To run on the KNL nodes, use the following job script, replacing with the desired job executable name. To run vasp after loading the proper module, use vasp_std, vasp_gam, or vasp_ncl. #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks=64 #Use 64 tasks for the job #SBATCH --qos=lsdi_knl2_normal #Set job to normal qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2_knl #Submit to the KNL nodes owned by the Persson Group #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable> Running on Haswell Nodes(on Low Priority) To run on Haswell nodes, use the following slurm submission script: #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks_per_core=1 #Use 1 task per core on the node #SBATCH --qos=savio_lowprio #Set job to low priority qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2 #Submit to the haswell nodes #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable> Peregrine NREL's supercomputer Peregrine is reserved for projects and jobs related to silicon-based Li-ion battery research. Getting an account on Peregrine To request a peregrine account, visit the following link. https://www.nrel.gov/hpc/user-accounts.html Additional resources: Other Persson group members and the NERSC website are both excellent resources for getting additional help. If that fails, you can reach out to the NERSC Operations staff: 1-800-666-3772 (or 1-510-486-8600) Computer Operations = menu option 1 (24/7) Account Support = menu option 2, accounts@nersc.gov HPC Consulting = menu option 3, or consult@nersc.gov Online Help Desk = help.nersc.gov Authors: Kara Fong, Eric Sivonxay, John Dagdelen Contact: karafong@lbl.gov","title":"HPC"},{"location":"computing/hpc/#computing-resources","text":"Our group\u2019s main computing resources are: NERSC (the LBNL supercomputing center, one of the biggest in the world) Lawrencium / Berkeley Research Computing Peregrine (the NREL supercomputing center) Argonne Leadership Computing Facility (sometimes) Oak Ridge Leadership Computing Facility (sometimes) At any time, if you feel you are computing-limited, please contact Kristin so she can work with you on finding solutions.","title":"Computing Resources "},{"location":"computing/hpc/#nersc","text":"","title":"NERSC "},{"location":"computing/hpc/#to-get-started-with-calculations-at-nersc","text":"Ask Kristin about whether you will be running at NERSC and, if so, under what account / repository to charge. Request a NERSC account through the NERSC homepage (Google \u201cNERSC account request\u201d). A NERSC Liason or PI Proxy will validate your account and assign you an initial allocation of computing hours. At this point, you should be able to log in, check CPU-hour balances, etc. through \u201cNERSC NIM\u201d and \u201cMy NERSC\u201d portals In order to log in and run jobs on the various machines at NERSC, review the NERSC documentation. In order to load and submit scripts for various codes (VASP, ABINIT, Quantum Espresso), NERSC has lots of information to help. Try Google, e.g. \u201cNERSC VASP\u201d. ... * Note that for commercial codes such as VASP, there is an online form that allows you to enter your VASP license, which NERSC will confirm and then allow you access to. Please make a folder inside your project directory and submit all your jobs there, as your home folder has only about 40GB of space. For example, for matgen project, your work folder path should be something like the following: /global/project/projectdirs/matgen/YOUR_NERSC_USERNAME You can also request a mongo database for your project to be hosted on NERSC. Google \u201cMongoDB on NERSC\u201d for instructions. Donny Winston or Patrick Huck can also help you get set up and provide you with a preconfigured database suited for running Materials Project style workflows.","title":"To get started with calculations at NERSC: "},{"location":"computing/hpc/#running-jobs-on-nersc","text":"This tutorial provides a brief overview of setting yourself up to run jobs on NERSC. If any information is unclear or missing, feel free to edit this document or contact Kara Fong.","title":"Running Jobs on NERSC"},{"location":"computing/hpc/#setting-up-a-nersc-account","text":"Contact the group\u2019s NERSC Liaison (currently Eric Sivonxay, see Group Jobs list). They will help you create an account and allocate you computational resources. You will then receive an email with instructions to fill out the Appropriate Use Policy form, set up your password, etc. Once your account is set up, you can manage it at the NERSC Information Management (NIM) website.","title":"Setting up a NERSC account:"},{"location":"computing/hpc/#connecting-with-ssh","text":"You must use the SSH protocol to connect to NERSC. Make sure you have SSH installed on your local computer (you can check this by typing which ssh). Make sure you have a directory named $HOME/.ssh on your local computer (if not, make it). Run the command ssh-keygen -t rsa -b 4096. This will generate an RSA key, which you can view in the file id_rsa.pub You\u2019ll be asked to enter a passphrase, which should be different from your password. You must store your SSH public key on the NERSC NIM database. Go to the NIM website, navigate to \u201cMy Stuff\u201d -> \u201cMy SSH Keys\u201d. Click on the SSH Keys tab. Copy your key (from id_rsa.pub) into the website\u2019s text box, click Add.","title":"Connecting with SSH:"},{"location":"computing/hpc/#logging-on","text":"Log on to Cori, for example, by submitting the following command in the terminal: ssh username@cori.nersc.gov You will be prompted to enter your passphrase. This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias cori=\"ssh your_username@cori.nersc.gov\" Now you will be able to initialize a SSH connection to cori just by typing cori in the command line and pressing enter.","title":"Logging on:"},{"location":"computing/hpc/#transferring-files-tofrom-nersc","text":"For small files, you can use SCP (secure copy). To get a file from NERSC, use: scp user_name@dtn01.nersc.gov:/remote/path/myfile.txt /local/path To send a file to NERSC, use: scp /local/path/myfile.txt user_name@dtn01.nersc.gov:/remote/path To move a larger quantity of data using a friendlier interface, use Globus Online.","title":"Transferring files to/from NERSC:"},{"location":"computing/hpc/#running-and-monitoring-jobs","text":"The following instructions are for running on Cori. Analogous information for running on Edison can be found here. Most jobs are run in batch mode, in which you prepare a shell script telling the batch system how to run the job (number of nodes, time the job will run, etc.). NERSC\u2019s batch system software is called SLURM. Below is a simple batch script example, copied from the NERSC website: #!bin/bash -l #SBATCH -N 2 #Use 2 nodes #SBATCH -t 00:30:00 #Set 30 minute time limit #SBATCH -q regular #Submit to the regular QOS #SBATCH -L scratch #Job requires $SCRATCH file system #SBATCH -C haswell #Use Haswell nodes srun -n 32 -c 4 ./my_executable Here, the first line specifies which shell to use (in this case bash). The keyword #SBATCH is used to start directive lines ( click here for a full description of the sbatch options you can specify). The word \u201csrun\u201d starts execution of the code. To submit your batch script, use sbatch myscript.sl in the directory containing the script file. Below are some useful commands to control and monitor your jobs: sqs -u username (Lists jobs for your account) scancel jobid (Cancels a job from the queue)","title":"Running and monitoring jobs:"},{"location":"computing/hpc/#choosing-a-qos-quality-of-service","text":"You specify which queue to use in your batch file. Use the debug queue for small, short test runs, the regular queue for production runs, and the premium queue for high-priority jobs.","title":"Choosing a QOS (quality of service):"},{"location":"computing/hpc/#choosing-a-node-type-haswell-vs-knl","text":"You may also specify the resource type you would like your job to run on, witnin your batch file. When running on Cori, there are two CPU architectures available, Haswell, and Knights Landing (known as KNL). Running on a Haswell node will afford you high individual core performance with up to 32 cores per node (or 64 threads per node). A KNL node provides a large core count (68 cores per node or 272 threads per node) which is suitable for programs capable of effectively utilizing multithreading. On Cori, there are 2388 Haswell nodes and 9688 KNL nodes. [TODO: FILL OUT THIS SECTION MORE]","title":"Choosing a node type (haswell vs knl):"},{"location":"computing/hpc/#automatic-job-submission-on-nersc-crontab","text":"In order to automatically manage job submission at NERSC, you can use crontab. You can submit jobs periodically even when you are not signed in to any NERSC systems and perhaps reduce the queue time from 5-10 days to a few hours. This is possible because of the way jobs are managed in atomate/fireworks. Please make sure you feel comfortable submitting individual jobs via atomate before reading this section. In atomate, by using --maxloop 3 for example when setting rocket_launch in your my_qadapter.yaml, after 3 trials in each minute if there are still no READY jobs available in your Launchpad Fireworks would stop the running job on NERSC to avoid wasting computing resources. On the other hand, if you have Fireworks available with the READY state and you have been using crontab for a few days, even if the jobs you submitted a few days ago start running on NERSC, they would pull any READY Fireworks and start RUNNING them reducing the turnaround from a few days to a few hours! So how to setup crontab? Please follow the instructions here: 1. ssh to the node where you want to setup the crontab; try one that is easy to remember such as cori01 or edison01; for logging in to a specific node just do for example \u201cssh cori01\u201d after you log in to the system (Cori in this example). Type and enter: crontab -e Now you can setup the following command in the opened vi editor. What it does is basically running the SCRIPT.sh file every 120 minutes of every day of every week of every month of every year (or simply /120 * * *): */120 * * * * /bin/bash -l PATH_TO_SCRIPT.sh >> PATH_TO_LOGFILE Setup your SCRIPT.sh like the following: (as a suggestion, you can simply put this file and the log file which keeps a log of submission states in your home folder): source activate YOUR_PRODUCTION_CONDA_ENVIRONMENT FW_CONFIG_FILE=PATH_TO_CONFIG_DIR/FW_config.yaml cd PATH_TO_YOUR_PRODUCTION_FOLDER qlaunch --fill_mode rapidfire -m 1000 --nlaunches 1 The last line of this 3-line file is really what submitting your job inside your production folder with the settings that you set in FW_config.yaml file. See atomate documentation for more info. Please make sure to set your PRODUCTION_FOLDER under /global/project/projectdirs/ that has much more space than your home folder and it is also backed up. Make sure to keep an eye on how close you are to disk space and file number limitations by checking https://my.nersc.gov/ periodically.","title":"Automatic job submission on NERSC: crontab "},{"location":"computing/hpc/#running-jupyter-notebooks-on-cori","text":"Jupyter notebooks are quickly becoming an indispensable tool for doing computational science. In some cases, you might want to (or need to) harness NERSC computing power inside of a jupyter notebook. To do this, you can use NERSC\u2019s new Jupyterhub system at https://jupyter-dev.nersc.gov/. These notebooks are run on a large memory node of Cori and can also submit jobs to the batch queues (see http://bit.ly/2A0mqrl for details). All of your files and the project directory will be accessible from the Jupyterhub, but your conda envs won\u2019t be available before you do some configuration. To set up a conda environment so it is accessible from the Jupyterhub, activate the environment and setup an ipython kernel. To do this, run the command \u201cpip install ipykernel\u201d. More info can be found at http://bit.ly/2yoKAzB.","title":"Running Jupyter Notebooks on Cori "},{"location":"computing/hpc/#automatic-job-packing-with-fireworks","text":"DISCLAIMER: Only use job packing if you have trouble with typical job submission. The following tip is not 100% guaranteed to work., and is based on limited, subjective experience on Cori. Talk to Alex Dunn (ardunn@lbl.gov) for help if you have trouble. The Cori queue system can be unreasonably slow when submitting many (e.g., hundreds, thousands) of small (e.g., single node or 2 nodes) jobs with qos-normal priority on Haswell. In practice, we have found that the Cori job scheduler will give your jobs low throughput if you have many jobs in queue, and you will often only be able to run 5-30 jobs at a time, while the rest wait in queue for far longer than originally expected (e.g., weeks). While there is no easy way to increase your queue submission rate (AFAIK), you can use FireWorks job-packing to \u201ctrick\u201d Cori\u2019s SLURM scheduler into running many jobs in serial on many parallel compute nodes with a single queue submission, vastly increasing throughput. You can use job packing with the \u201cmulti\u201d option to rlaunch. This command launches N parallel python processes on the Cori scheduling node, each which runs a job using M compute nodes. The steps to job packing are: 1. Edit your my_qadapter.yaml file to reserve N * M nodes for each submission. For example, if each of your jobs takes M = 2 nodes, and you want a N = 10 x speedup, reserve 20 nodes per queue submission. 2. Change your rlaunch command to: rlaunch -c /your/config multi N To have each FireWorks process run as many jobs as possible in serial before the walltime, use the --nlaunches 0 option. To prevent FireWorks from submitting jobs with little walltime left (causing jobs to frequently get stuck as \u201cRUNNING\u201d), set the --timeout option. Make sure --timeout is set so that even a long running job submitted at the end of your allocation will not run over your walltime limit. Your my_qadapter.yaml should then have something similar to the following lines: rocket_launch: rlaunch -c /your/config multi 10 --nlaunches 0 --timeout 169200 nodes: 20 Typically, setting N <= 10 will give you a good N-times speedup with no problems. There are no guarantees, however, when N > 10-20. Use N > 50 at your own risk!","title":"Automatic Job Packing with FireWorks "},{"location":"computing/hpc/#berkeley-research-computing","text":"Berkeley Research Computing (BRC) hosts the Savio supercomputing cluster. Savio operates on a condo computing model, in which many PI's and researchers purchase nodes to add to the system. Nodes are accessible to all who have access to the system, though priority access will be given to contributors of the specific nodes. BRC provides 3 types of allocations: Condo - Priority access for nodes contributed by the condo group. Faculty Computing Allowance (FCA) - Limited computing time provided to each Faculty member using Savio.","title":"Berkeley Research Computing "},{"location":"computing/hpc/#setting-up-a-brc-account","text":"To get an account on Savio, fill out the form linked below, making sure to select the appropriate allocation. Typically, most students and postdocs will be running on co_lsdi. http://research-it.berkeley.edu/services/high-performance-computing/getting-account After your account is made, you'll need to set up 2-factor authentication. We recommend using Google Authenticator, although any OTP manager will work. [TODO: Fill out this with specific 2-factor setup details]","title":"Setting up a BRC account"},{"location":"computing/hpc/#logging-into-brc","text":"To access your shiny new savio account, you'll want to SSH onto the system from a terminal. ssh username@hpc.brc.berekeley.edu You will be prompted to enter your passphrase. This will take you to your home directory. You may also find it useful to set up an alias for signing on to HPC resources. To do this, add the following line to your bash_profile: alias savio=\"ssh username@hpc.brc.berekeley.edu\"","title":"Logging into BRC"},{"location":"computing/hpc/#running-on-brc","text":"Under the condo account co_lsdi, we have exclusive access to 28 KNL nodes. Additionally, we have the ability to run on other nodes at low priority mode.","title":"Running on BRC"},{"location":"computing/hpc/#accessing-software-binaries","text":"Software within BRC is managed through modules. You can access precompiled, preinstalled software by loading the desired module. module load <module_name> To view a list of currently installed programs, use the following command: module avail To view the currently loaded modules use the command: module list Software modules can be removed by using either of the following two commands: module unload <module_name> module purge","title":"Accessing Software binaries"},{"location":"computing/hpc/#accessing-in-house-software-packages","text":"The Persson Group maintains their own versions of popular codes such as VASP, GAUSSIAN, QCHEM and LAMMPS. To access these binaries, ensure that you have the proper licenses and permissions, then append the following line to the .bashrc file in your root directory: export MODULEPATH=${MODULEPATH}:/global/home/groups/co_lsdi/sl7/modfiles","title":"Accessing In-House software packages"},{"location":"computing/hpc/#using-persson-group-owned-knl-nodes","text":"To run on the KNL nodes, use the following job script, replacing with the desired job executable name. To run vasp after loading the proper module, use vasp_std, vasp_gam, or vasp_ncl. #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks=64 #Use 64 tasks for the job #SBATCH --qos=lsdi_knl2_normal #Set job to normal qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2_knl #Submit to the KNL nodes owned by the Persson Group #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable>","title":"Using Persson Group Owned KNL nodes"},{"location":"computing/hpc/#running-on-haswell-nodeson-low-priority","text":"To run on Haswell nodes, use the following slurm submission script: #!bin/bash -l #SBATCH --nodes=1 #Use 1 node #SBATCH --ntasks_per_core=1 #Use 1 task per core on the node #SBATCH --qos=savio_lowprio #Set job to low priority qos #SBATCH --time=01:00:00 #Set 1 hour time limit for job #SBATCH --partition=savio2 #Submit to the haswell nodes #SBATCH --account=co_lsdi #Charge to co_lsdi accout #SBATCH --job-name=savio2_job #Name for the job mpirun --bind-to core <executable>","title":"Running on Haswell Nodes(on Low Priority)"},{"location":"computing/hpc/#peregrine","text":"NREL's supercomputer Peregrine is reserved for projects and jobs related to silicon-based Li-ion battery research.","title":"Peregrine "},{"location":"computing/hpc/#getting-an-account-on-peregrine","text":"To request a peregrine account, visit the following link. https://www.nrel.gov/hpc/user-accounts.html","title":"Getting an account on Peregrine"},{"location":"computing/hpc/#additional-resources","text":"Other Persson group members and the NERSC website are both excellent resources for getting additional help. If that fails, you can reach out to the NERSC Operations staff: 1-800-666-3772 (or 1-510-486-8600) Computer Operations = menu option 1 (24/7) Account Support = menu option 2, accounts@nersc.gov HPC Consulting = menu option 3, or consult@nersc.gov Online Help Desk = help.nersc.gov Authors: Kara Fong, Eric Sivonxay, John Dagdelen Contact: karafong@lbl.gov","title":"Additional resources:"},{"location":"computing/software/","text":"Our software stack A brief summary of our software stack includes: pymatgen / pymatgen-db - for representing and analyzing crystal structures, as well as setting up/performing manual calculations FireWorks - for executing and managing calculation workflows at supercomputing centers custodian - instead of directly running an executable like VASP, one can wrap the executable in custodian to detect and fix errors atomate - for quickly defining multiple types of materials science workflows matminer - for large data analysis and visualization We also heavily use the Materials Project database. Additional software used by many members of our group include: VESTA - Ovito - VMD - Studio 3T - PyCharm - iTerm2 - To learn how to use the software stack, you can consult the documentation of the individual codebases as well as review the following resources: The 2018 Materials Project workshop (note that MatMethods is now called atomate): https://github.com/materialsproject/workshop-2018 The 2014 Materials Virtual Lab presentations: https://materialsvirtuallab.org/software/ The Materials Project YouTube tutorials: https://www.youtube.com/user/MaterialsProject If you have a specific question, sometimes the easiest solution is to post it to the Slack group and crowdsource the answer (or just ask Shyam).","title":"Software"},{"location":"computing/software/#our-software-stack","text":"A brief summary of our software stack includes: pymatgen / pymatgen-db - for representing and analyzing crystal structures, as well as setting up/performing manual calculations FireWorks - for executing and managing calculation workflows at supercomputing centers custodian - instead of directly running an executable like VASP, one can wrap the executable in custodian to detect and fix errors atomate - for quickly defining multiple types of materials science workflows matminer - for large data analysis and visualization","title":"Our software stack"},{"location":"computing/software/#we-also-heavily-use-the-materials-project-database","text":"Additional software used by many members of our group include: VESTA - Ovito - VMD - Studio 3T - PyCharm - iTerm2 - To learn how to use the software stack, you can consult the documentation of the individual codebases as well as review the following resources: The 2018 Materials Project workshop (note that MatMethods is now called atomate): https://github.com/materialsproject/workshop-2018 The 2014 Materials Virtual Lab presentations: https://materialsvirtuallab.org/software/ The Materials Project YouTube tutorials: https://www.youtube.com/user/MaterialsProject If you have a specific question, sometimes the easiest solution is to post it to the Slack group and crowdsource the answer (or just ask Shyam).","title":"We also heavily use the Materials Project database."},{"location":"getting_started/new_computer/","text":"Getting a Computer Most long-term appointments (graduate student, postdoc, staff) will mean purchasing a new computer. Short-term appointments (e.g., internships) will not involve a computer purchase unless otherwise stated - you can instead receive an excellent computer from the group\u2019s stock. Mac, Windows, or Linux? You are advised to buy a Mac, and probably a Macbook Pro. In our experience these are the best systems for our type of work. Purchasing LBNL Funded Members Use LBNL Ebuy (not Ebay) wherever possible - you need to be on the lab network (onsite via an ethernet cable) or be connected via the VPN Use Amazon, etc. to buy various components if not available via EBuy The laptop is government property; you are expected to return it to the group when you are done working at LBNL. Note that Mac computers make it very simple to transfer everything over to your next computer. You are free to take your laptop home, on trips, etc., unless you are an intern in which case other restrictions may apply from the internship program. The lab receives your computer and tags it before sending it over to you. You must back up your computer very regularly (at least once per week, ideally continuously). This is simple using the Time Machine app. Just plug your backup drive into your monitor so when you connect to your monitor, you also back up. If there are (for some reason) errors in backing up, fix that issue immediately. There are zero excuses for not doing this. UCB Funded Members Use BearBuy More details coming to this documentation soon. Selecting a computer, monitor, and accessories Your computer workstation is one area where you should get whatever you think will make you most productive. As you'll be working on this setup the majority of your time in the group, please don't worry too much about the cost here. For the computer, you should select a Macbook Pro (any screen size) as mentioned above. You can use the Apple website to browse details. Many of us use a 13\u201d Macbook Pro. It is powerful enough to do serious work and light/small enough to use on a plane. A 15\u201d Macbook Pro is also a good choice. If you would like to get anything other than a Macbook Pro, talk to some senior members of the group first to get their input. For the monitor, a number of us use a single Thunderbolt display. While this is no longer available for purchase, there may be one or two floating around the group that you can adopt. Another good option available is the LG 27MU88-W (4K resolution) monitor which is on Ebuy. Note that one big screen is usually better ergonomically than dual monitors, and you can use the \u201cSpaces\u201d feature of Mac OS/X to quickly flip between virtual screens if needed. If you'd like a second monitor, we have a number of them floating around the group and one can probably be found for you. For accessories, make sure to get: * An extra charger * A VGA adapter dongle * An ethernet cable adapter dongle * A Time Machine hard disk (for backup). 4TB is a good size. There is a usb-c G-Drive that also doubles as a charger that some of us enjoy. * A keyboard. The Apple Wireless Keyboard is a good option. Others prefer mechanical keyboards and some prefer to use the loudest keyboard that they can find (Shyam). If you prefer a larger or ergonomic keyboard, you can certainly get that. * A mouse/trackpad. We suggest Apple Magic Trackpad because Mac OS has customized a lot of the interface for the trackpad (e.g., gestures). Some also value consistency between their laptop and desk workstation. After a while you get used to doing everything on your trackpad even if you were previously very productive/accurate with a mouse on Windows. However, many of us get by just fine with a mouse (especially the Apple Magic Mouse, which has some gesture support.) * (optional) A presentation tool, e.g., Logitech R800. Making the purchase Provide all the details of your selections in an email and send to Alice Mueller. If all looks ok, she will give you a project and activity ID. Go to eBuy, and for items available there, add them to your cart and submit the requisition with the project and activity ID. Ask Alice Mueller about which SAS approver to list if you are unsure (the SAS approver can vary by project and activity ID). For items not available on eBuy, contact esdradmin@lbl.gov (and cc Kristin and Alice) to obtain a procurement form. Fill it out with item details (Vendor, website, price, etc.) and send it back. If you select the overnight shipping option (ask Kristin about this and the related extra costs) most parts, except the computer, will arrive within a week to 10 days. The computer needs to be tagged by the lab, so with overnight shipping, it should arrive within 2 weeks. Ideally, you will select your computer well before arriving at the lab and won\u2019t need overnight shipping.","title":"Purchasing a Computer"},{"location":"getting_started/new_computer/#getting-a-computer","text":"Most long-term appointments (graduate student, postdoc, staff) will mean purchasing a new computer. Short-term appointments (e.g., internships) will not involve a computer purchase unless otherwise stated - you can instead receive an excellent computer from the group\u2019s stock.","title":"Getting a Computer "},{"location":"getting_started/new_computer/#mac-windows-or-linux","text":"You are advised to buy a Mac, and probably a Macbook Pro. In our experience these are the best systems for our type of work.","title":"Mac, Windows, or Linux?"},{"location":"getting_started/new_computer/#purchasing","text":"","title":"Purchasing"},{"location":"getting_started/new_computer/#lbnl-funded-members","text":"Use LBNL Ebuy (not Ebay) wherever possible - you need to be on the lab network (onsite via an ethernet cable) or be connected via the VPN Use Amazon, etc. to buy various components if not available via EBuy The laptop is government property; you are expected to return it to the group when you are done working at LBNL. Note that Mac computers make it very simple to transfer everything over to your next computer. You are free to take your laptop home, on trips, etc., unless you are an intern in which case other restrictions may apply from the internship program. The lab receives your computer and tags it before sending it over to you. You must back up your computer very regularly (at least once per week, ideally continuously). This is simple using the Time Machine app. Just plug your backup drive into your monitor so when you connect to your monitor, you also back up. If there are (for some reason) errors in backing up, fix that issue immediately. There are zero excuses for not doing this.","title":"LBNL Funded Members"},{"location":"getting_started/new_computer/#ucb-funded-members","text":"Use BearBuy More details coming to this documentation soon.","title":"UCB Funded Members"},{"location":"getting_started/new_computer/#selecting-a-computer-monitor-and-accessories","text":"Your computer workstation is one area where you should get whatever you think will make you most productive. As you'll be working on this setup the majority of your time in the group, please don't worry too much about the cost here. For the computer, you should select a Macbook Pro (any screen size) as mentioned above. You can use the Apple website to browse details. Many of us use a 13\u201d Macbook Pro. It is powerful enough to do serious work and light/small enough to use on a plane. A 15\u201d Macbook Pro is also a good choice. If you would like to get anything other than a Macbook Pro, talk to some senior members of the group first to get their input. For the monitor, a number of us use a single Thunderbolt display. While this is no longer available for purchase, there may be one or two floating around the group that you can adopt. Another good option available is the LG 27MU88-W (4K resolution) monitor which is on Ebuy. Note that one big screen is usually better ergonomically than dual monitors, and you can use the \u201cSpaces\u201d feature of Mac OS/X to quickly flip between virtual screens if needed. If you'd like a second monitor, we have a number of them floating around the group and one can probably be found for you. For accessories, make sure to get: * An extra charger * A VGA adapter dongle * An ethernet cable adapter dongle * A Time Machine hard disk (for backup). 4TB is a good size. There is a usb-c G-Drive that also doubles as a charger that some of us enjoy. * A keyboard. The Apple Wireless Keyboard is a good option. Others prefer mechanical keyboards and some prefer to use the loudest keyboard that they can find (Shyam). If you prefer a larger or ergonomic keyboard, you can certainly get that. * A mouse/trackpad. We suggest Apple Magic Trackpad because Mac OS has customized a lot of the interface for the trackpad (e.g., gestures). Some also value consistency between their laptop and desk workstation. After a while you get used to doing everything on your trackpad even if you were previously very productive/accurate with a mouse on Windows. However, many of us get by just fine with a mouse (especially the Apple Magic Mouse, which has some gesture support.) * (optional) A presentation tool, e.g., Logitech R800.","title":"Selecting a computer, monitor, and accessories"},{"location":"getting_started/new_computer/#making-the-purchase","text":"Provide all the details of your selections in an email and send to Alice Mueller. If all looks ok, she will give you a project and activity ID. Go to eBuy, and for items available there, add them to your cart and submit the requisition with the project and activity ID. Ask Alice Mueller about which SAS approver to list if you are unsure (the SAS approver can vary by project and activity ID). For items not available on eBuy, contact esdradmin@lbl.gov (and cc Kristin and Alice) to obtain a procurement form. Fill it out with item details (Vendor, website, price, etc.) and send it back. If you select the overnight shipping option (ask Kristin about this and the related extra costs) most parts, except the computer, will arrive within a week to 10 days. The computer needs to be tagged by the lab, so with overnight shipping, it should arrive within 2 weeks. Ideally, you will select your computer well before arriving at the lab and won\u2019t need overnight shipping.","title":"Making the purchase"},{"location":"getting_started/newcomers/","text":"Before You Arrive Although many things can only be taken care of after arriving at LBNL, here are a few simple things you should do in advance. Join Slack Slack is a messaging app that we use extensively to communicate both within our group and with our Materials Project collaborators. To join our slack channel , you'll need to use your @lbl.gov email address. Order a Computer Postdocs and graduate students - let\u2019s order your workstation in advance so that it\u2019s ready by the time you arrive. How? See getting a computer. Useful Onboarding Points of Contact and Quick Links LBL Administrative Assistance : Alice Mueller, 33-122B Desk Assignments + Group Inventory : Rebecca Stern, 33-143F Persson Google Group (email lists, drive, etc.) : Donny Winston Group Meeting : Trevor Seguin Persson Group Website : People Internal Group Site lbl.gov A-Z Index After You Arrive Welcome to Berkeley! Here is a checklist to help you get started with joining the Persson Research Group. First Day Checklist (Very Basics & High Priority Administrative Tasks after HR Orientation) Once you make your way inside building 33, ask around to find someone in the Persson Group who can help you find your desk and provide a short walk through to become familiar with the building and introduce you to other group members. Follow the instructions received during your orientation to activate your Berkeley Lab Identity Account (aka LDAP). This log-in will be used for most LBL online resources. Connect to the internet using the lbnl-visitor WiFi network (it is open access). Try out your newly created LDAP log-in. Connect to the lbnl-employee WiFi network , open your @lbl.gov email, google calendar, etc. Send out an email introducing yourself to perssongroup@lbl.gov (this is the email list for everyone in the Persson group) from your @lbl.gov email address. This is a new tradition to help welcome new group members since it can take some time to meet everyone in person. This also serves as a notice for the group to help initiate various onboarding tasks. Here is the intro email questions template and an example introduction email . Ask Kristin/Alice to find our what project and activity id is associated with your funding and should be used for payroll, purchasing, etc. You will also want to find the associated SAS Approver from Alice or another group member with the same funding source. See Alice with your Employee ID to introduce yourself and set-up B33 after hours access. By default, you will not have off-hours site access to building 33, i.e., on weekends, holidays, and from 5:30pm to 7am on weekdays. Prioritize completing the GERT (General Radiological Worker Training) before the end of your first day. Begin setting up your desk (ideally before completing the ergonomic self-assessment). Here are some tips. Order a computer as soon as possible if you have not done so already. See Getting a Computer for advice on selecting a computer and needed accessories. You are welcome to ask Alice or a group member for a purchasing overview or help with Ebuy. First Week Checklist (also start the below Research Computer & IT Checklist when possible) Send an email to Donny Winston requesting to be added to the Persson Group email lists and team google drive (if you do not have access already). Use the subject line \"Onboarding: \". Complete all LBL/UC Berkeley training courses including the ergonomic self-assessment. Note the GERT must be completed on your first day. Orient yourself with LBL's calendar system (Google Calendar). Update your LBL calendar with your schedule so other group members can view your availability. Check to ensure your calendar is populated with needed meetings (group meetings, subgroup meetings, etc.) Contact a meeting organizer to invite you to needed calendar events. Send an email to Sang-Won Park with a headshot and tiny bio for the group website (bit of humor in the tiny bio is very welcome). Explore the group\u2019s Google Drive folder. Note some of these documents populate the group's internal website. Add your name and information to the Persson Group roster Work with Alice Muller to update your name tag outside your office. Research Introduction Checklist Meet with Kristin to plan an appropriate introduction for your specific research, project management, subgroup, etc. Ask to read the proposal that funds your work. This will help explain the impact of your project, the long-term plans and goals, and how your project fits in with other efforts. Get familiar with your subgroup and any other individuals you will be working closely with for your research. Generally take the time to learn about other group member's research focuses and areas of expertise to get a sense of who may be your best resource for different questions and who you may be supporting if you are a more senior group member. Computer & IT Checklist Set up your research computer. See setting up you computer on how to do this and some recommended software to install. Install the lab VPN for connecting to the lab network from home. For example, this lets you download research articles from home. See https://software.lbl.gov for instructions on installation. Obtain user accounts for any computing resources (LINK) you may be using. Obtain a license for any software packages you might be using. For example, you may need to be added to the VASP users list (for VASP, you should also register for the forum.) Get connected to the printer. The printer is located in 143C and instructions for how to connect can be found here. Julian Self is the current printer czar and can advise you if you are having any difficulties. Other Helpful Information to Get Started Getting situated in your workspace Please feel free to decorate your lab space with photos, posters, or other personal touches. Order any peripherals that will help you work more efficiently and comfortably including keyboards, mice, ergonomic devices such as wrist pads, and external displays. Authors: John Dagdelen Contact: jdagdelen@lbl.gov","title":"Newcomers"},{"location":"getting_started/newcomers/#before-you-arrive","text":"Although many things can only be taken care of after arriving at LBNL, here are a few simple things you should do in advance.","title":"Before You Arrive "},{"location":"getting_started/newcomers/#useful-onboarding-points-of-contact-and-quick-links","text":"LBL Administrative Assistance : Alice Mueller, 33-122B Desk Assignments + Group Inventory : Rebecca Stern, 33-143F Persson Google Group (email lists, drive, etc.) : Donny Winston Group Meeting : Trevor Seguin Persson Group Website : People Internal Group Site lbl.gov A-Z Index","title":"Useful Onboarding Points of Contact and Quick Links "},{"location":"getting_started/newcomers/#after-you-arrive","text":"Welcome to Berkeley! Here is a checklist to help you get started with joining the Persson Research Group.","title":"After You Arrive "},{"location":"getting_started/newcomers/#first-day-checklist-very-basics-high-priority-administrative-tasks-after-hr-orientation","text":"Once you make your way inside building 33, ask around to find someone in the Persson Group who can help you find your desk and provide a short walk through to become familiar with the building and introduce you to other group members. Follow the instructions received during your orientation to activate your Berkeley Lab Identity Account (aka LDAP). This log-in will be used for most LBL online resources. Connect to the internet using the lbnl-visitor WiFi network (it is open access). Try out your newly created LDAP log-in. Connect to the lbnl-employee WiFi network , open your @lbl.gov email, google calendar, etc. Send out an email introducing yourself to perssongroup@lbl.gov (this is the email list for everyone in the Persson group) from your @lbl.gov email address. This is a new tradition to help welcome new group members since it can take some time to meet everyone in person. This also serves as a notice for the group to help initiate various onboarding tasks. Here is the intro email questions template and an example introduction email . Ask Kristin/Alice to find our what project and activity id is associated with your funding and should be used for payroll, purchasing, etc. You will also want to find the associated SAS Approver from Alice or another group member with the same funding source. See Alice with your Employee ID to introduce yourself and set-up B33 after hours access. By default, you will not have off-hours site access to building 33, i.e., on weekends, holidays, and from 5:30pm to 7am on weekdays. Prioritize completing the GERT (General Radiological Worker Training) before the end of your first day. Begin setting up your desk (ideally before completing the ergonomic self-assessment). Here are some tips. Order a computer as soon as possible if you have not done so already. See Getting a Computer for advice on selecting a computer and needed accessories. You are welcome to ask Alice or a group member for a purchasing overview or help with Ebuy.","title":"First Day Checklist (Very Basics &amp; High Priority Administrative Tasks after HR Orientation) "},{"location":"getting_started/newcomers/#first-week-checklist-also-start-the-below-research-computer-it-checklist-when-possible","text":"Send an email to Donny Winston requesting to be added to the Persson Group email lists and team google drive (if you do not have access already). Use the subject line \"Onboarding: \". Complete all LBL/UC Berkeley training courses including the ergonomic self-assessment. Note the GERT must be completed on your first day. Orient yourself with LBL's calendar system (Google Calendar). Update your LBL calendar with your schedule so other group members can view your availability. Check to ensure your calendar is populated with needed meetings (group meetings, subgroup meetings, etc.) Contact a meeting organizer to invite you to needed calendar events. Send an email to Sang-Won Park with a headshot and tiny bio for the group website (bit of humor in the tiny bio is very welcome). Explore the group\u2019s Google Drive folder. Note some of these documents populate the group's internal website. Add your name and information to the Persson Group roster Work with Alice Muller to update your name tag outside your office.","title":"First Week Checklist (also start the below Research Computer &amp; IT Checklist when possible) "},{"location":"getting_started/newcomers/#research-introduction-checklist","text":"Meet with Kristin to plan an appropriate introduction for your specific research, project management, subgroup, etc. Ask to read the proposal that funds your work. This will help explain the impact of your project, the long-term plans and goals, and how your project fits in with other efforts. Get familiar with your subgroup and any other individuals you will be working closely with for your research. Generally take the time to learn about other group member's research focuses and areas of expertise to get a sense of who may be your best resource for different questions and who you may be supporting if you are a more senior group member.","title":"Research Introduction Checklist "},{"location":"getting_started/newcomers/#computer-it-checklist","text":"Set up your research computer. See setting up you computer on how to do this and some recommended software to install. Install the lab VPN for connecting to the lab network from home. For example, this lets you download research articles from home. See https://software.lbl.gov for instructions on installation. Obtain user accounts for any computing resources (LINK) you may be using. Obtain a license for any software packages you might be using. For example, you may need to be added to the VASP users list (for VASP, you should also register for the forum.) Get connected to the printer. The printer is located in 143C and instructions for how to connect can be found here. Julian Self is the current printer czar and can advise you if you are having any difficulties.","title":"Computer &amp; IT Checklist "},{"location":"getting_started/newcomers/#other-helpful-information-to-get-started","text":"","title":"Other Helpful Information to Get Started "},{"location":"getting_started/newcomers/#getting-situated-in-your-workspace","text":"Please feel free to decorate your lab space with photos, posters, or other personal touches. Order any peripherals that will help you work more efficiently and comfortably including keyboards, mice, ergonomic devices such as wrist pads, and external displays. Authors: John Dagdelen Contact: jdagdelen@lbl.gov","title":"Getting situated in your workspace "},{"location":"getting_started/purchasing/","text":"","title":"Purchasing"},{"location":"students/prelims/","text":"","title":"Preparing for Prelims"},{"location":"students/qualifying/","text":"","title":"Qualifying Exams"},{"location":"students/residency/","text":"Information for Graduate Students New students coming from another state/country It's important that you establish California residency early in your first semester. If you don't complete the necessary tasks on time, you will be classified as a out-of-state student for the following academic year. Although the deadline for filing your Statement of Legal Residence (SLR) is in June, you must complete all necessary conditions for residency (getting your CA drivers license, registering your car in CA, etc) before the end of your first semester. To satisfy the union of physical presence and intent, legal indicia of intent should be acquired and all out-of-state legal indicia relinquished at least 366 days prior to the term for which a resident classification is requested. However, the University allows a limited period of time within the 366-day requirement to obtain legal indicia of intent and relinquish all ties to the past place of residence. For students either newly enrolling or continuing enrollment at UC who are requesting a resident classification for the Fall 2019 term, California legal intent must have been established and all out-of-state legal intent relinquished prior to the end of the Fall 2018 term.","title":"Establishing Residency"},{"location":"students/residency/#information-for-graduate-students","text":"","title":"Information for Graduate Students "},{"location":"students/residency/#new-students-coming-from-another-statecountry","text":"It's important that you establish California residency early in your first semester. If you don't complete the necessary tasks on time, you will be classified as a out-of-state student for the following academic year. Although the deadline for filing your Statement of Legal Residence (SLR) is in June, you must complete all necessary conditions for residency (getting your CA drivers license, registering your car in CA, etc) before the end of your first semester. To satisfy the union of physical presence and intent, legal indicia of intent should be acquired and all out-of-state legal indicia relinquished at least 366 days prior to the term for which a resident classification is requested. However, the University allows a limited period of time within the 366-day requirement to obtain legal indicia of intent and relinquish all ties to the past place of residence. For students either newly enrolling or continuing enrollment at UC who are requesting a resident classification for the Fall 2019 term, California legal intent must have been established and all out-of-state legal intent relinquished prior to the end of the Fall 2018 term.","title":"New students coming from another state/country "}]}